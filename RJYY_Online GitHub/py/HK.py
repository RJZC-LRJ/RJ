# -*- coding: utf-8 -*-
# by @å—·å‘œ

from urllib.parse import urlparse, quote, unquote
from pprint import pprint
from base64 import b64decode, b64encode
from pyquery import PyQuery as pq
from Crypto.Util.Padding import unpad
from Crypto.Cipher import AES
from pyquery.pyquery import PyQuery
import colorsys
import json
import random
import re
import sys

import threading
import requests
import os
import hashlib
import time

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from base.spider import Spider


sys.path.append('..')



class WebDecryptor:
    def __init__(self):
        self.cookies = {}
    
    def is_unicode_encoded(self, text):
        """åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦æ˜¯Unicodeç¼–ç """
        return bool(re.search(r'\\u[0-9a-fA-F]{4}', text))
    
    def extract_play_source_src(self, html_content):
        """ç›´æ¥æå–playSource.srcçš„å€¼"""
        # ç›´æ¥æŸ¥æ‰¾playSource.srcçš„èµ‹å€¼
        pattern = r'playSource\s*=\s*\{[^}]*src\s*:\s*"([^"]*)"[^}]*\}'
        match = re.search(pattern, html_content, re.DOTALL)
        
        if match:
            return match.group(1)
        
        return None
    
    def extract_encrypted_string(self, html_content):
        """æå–åŠ å¯†å­—ç¬¦ä¸²"""
        # æŸ¥æ‰¾KKYS.safePlay().url()ä¸­çš„åŠ å¯†å­—ç¬¦ä¸²
        pattern = r'KKYS\.safePlay\(\)\.url\("([^"]+)"\)'
        match = re.search(pattern, html_content)
        
        if match:
            return match.group(1)
        
        return None
    
    def get_web_content(self, url):
        """è·å–ç½‘é¡µå†…å®¹"""
        try:
            headers = {
                "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                "accept-language": "zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6,ko;q=0.5",
                "cache-control": "no-cache",
                "pragma": "no-cache",
                "priority": "u=0, i",
                "sec-ch-ua": "\\Chromium;v=\\142, \\Microsoft",
                "sec-ch-ua-mobile": "?0",
                "sec-ch-ua-platform": "\\Windows",
                "sec-fetch-dest": "document",
                "sec-fetch-mode": "navigate",
                "sec-fetch-site": "same-origin",
                "sec-fetch-user": "?1",
                "upgrade-insecure-requests": "1",
                "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 Edg/142.0.0.0"
            }
            
            response = requests.get(url, headers=headers, cookies=self.cookies, timeout=10)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"è·å–ç½‘é¡µå†…å®¹å¤±è´¥: {e}")
            return None
    
    def extract_encrypted_value(self, html_content):
        """ä»HTMLä¸­æå–åŠ å¯†çš„å€¼"""
        pattern = r"window\.whatTMDwhatTMDPPPP\s*=\s*'([^']+)'"
        match = re.search(pattern, html_content)
        
        if match:
            return match.group(1)
        return None
    
    def decrypt_value_python(self, encrypted_value):
        """ä½¿ç”¨Pythonå®ç°AESè§£å¯†"""
        try:
            print(f"æ­£åœ¨ä½¿ç”¨Pythonè§£å¯†...")
            
            # AESå¯†é’¥
            key = b"Isu7fOAvI6!&IKpAbVdhf&^F"
            
            # Base64è§£ç 
            encrypted_bytes = b64decode(encrypted_value)
            
            # åˆ›å»ºAESè§£å¯†å™¨ (ECBæ¨¡å¼)
            cipher = AES.new(key, AES.MODE_ECB)
            
            # è§£å¯†
            decrypted_bytes = cipher.decrypt(encrypted_bytes)
            
            # å»é™¤PKCS7å¡«å……
            decrypted_bytes = unpad(decrypted_bytes, AES.block_size)
            
            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            result = decrypted_bytes.decode('utf-8')
            
            print(f"è§£å¯†å®Œæˆ: {result}")
            return result
            
        except Exception as e:
            print(f"Pythonè§£å¯†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def decrypt_value(self, encrypted_value):
        """è§£å¯†åŠ å¯†çš„å€¼ - å°è¯•ä¸¤ç§æ–¹æ³•"""
        # é¦–å…ˆå°è¯•Pythonè§£å¯†
        result = self.decrypt_value_python(encrypted_value)
        if result and not result.startswith("è§£å¯†å¤±è´¥"):
            return result
        
        # å¦‚æœPythonè§£å¯†å¤±è´¥ï¼Œå°è¯•execjsï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            import execjs
            # å¦‚æœæœ‰execjsï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ execjsçš„è§£å¯†ä»£ç 
            print("execjså¯ç”¨ï¼Œä½†Pythonè§£å¯†å·²å¤±è´¥")
            return None
        except ImportError:
            print("execjsä¸å¯ç”¨")
            return None
    
    def process_url(self, url):
        """å¤„ç†æ•´ä¸ªURLè§£å¯†æµç¨‹"""
        print(f"å¼€å§‹å¤„ç†URL: {url}")
        
        # 1. è·å–ç½‘é¡µå†…å®¹
        html_content = self.get_web_content(url)
        if not html_content:
            return None
        
        # 2. è·å–m3u8åœ°å€
        m3u8_url = self.extract_play_source_src(html_content)

        if not m3u8_url:
            print("åŠ å¯†äº† å¼€å§‹è§£å¯†")
            # æ‰¾ä¸åˆ°å°±æ˜¯åŠ å¯†äº†
            # 3. åŸæ¥çš„åŠŸèƒ½ï¼šæå–whatTMDwhatTMDPPPPå€¼å¹¶è§£å¯†
            encrypted_value = self.extract_encrypted_value(html_content)
            if encrypted_value:
                m3u8_url = self.decrypt_value(encrypted_value)
                if m3u8_url and m3u8_url.startswith("è§£å¯†å¤±è´¥"):
                    print(f"âŒ è§£å¯†å¤±è´¥: {m3u8_url}")
                    return None
            else:
                print("âŒ æœªæ‰¾åˆ°åŠ å¯†å€¼")
                return None

        else:
            print("æœªåŠ å¯†")
            print(f"m3u8åœ°å€: {m3u8_url}")
            
        return {
            'url': url,
            'm3u8_url': m3u8_url,
        }



class Spider(Spider):

    def init(self, extend="{}"):
        self.domin = 'https://103.194.185.51:51123'
        self.proxies = {}
        self.headers = {
            'User-Agent': "Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Mobile Safari/537.36 Edg/142.0.0.0",
            'Accept': "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            'Accept-Encoding': "gzip, deflate, br, zstd",
            'pragma': "no-cache",
            'cache-control': "no-cache",
            'sec-ch-ua': "\"Chromium\";v=\"142\", \"Microsoft Edge\";v=\"142\", \"Not_A Brand\";v=\"99\"",
            'sec-ch-ua-mobile': "?1",
            'sec-ch-ua-platform': "\"Android\"",
            'upgrade-insecure-requests': "1",
            'sec-fetch-site': "none",
            'sec-fetch-mode': "navigate",
            'sec-fetch-user': "?1",
            'sec-fetch-dest': "document",
            'accept-language': "zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6,ko;q=0.5",
            'priority': "u=0, i",
        }

        self.host = "https://103.194.185.51:51123"
        self.headers.update({'Origin': self.host, 'Referer': f"{self.host}/"})

        # åˆå§‹åŒ–tå‚æ•°ç¼“å­˜
        self._t_param_cache = None
        self._t_param_time = 0
        self._t_param_timeout = 300  # 5åˆ†é’Ÿç¼“å­˜

        # é¢„åŠ è½½tå‚æ•°
        self._preload_t_parameter()
        
    def _preload_t_parameter(self):
        """åœ¨åˆå§‹åŒ–æ—¶é¢„åŠ è½½tå‚æ•°"""
        def preload_task():
            try:
                print("ğŸ”„ é¢„åŠ è½½tå‚æ•°...")
                t_value = self.get_t_parameter()
                if t_value:
                    self._t_param_cache = t_value
                    self._t_param_time = time.time()
                    print("âœ… tå‚æ•°é¢„åŠ è½½æˆåŠŸ")
                else:
                    print("âŒ tå‚æ•°é¢„åŠ è½½å¤±è´¥")
            except Exception as e:
                print(f"é¢„åŠ è½½tå‚æ•°å¼‚å¸¸: {e}")
        
        # åœ¨æ–°çº¿ç¨‹ä¸­é¢„åŠ è½½ï¼Œä¸é˜»å¡ä¸»åˆå§‹åŒ–
        thread = threading.Thread(target=preload_task)
        thread.daemon = True
        thread.start()
    def get_t_parameter_cached(self):
        """å¸¦ç¼“å­˜çš„tå‚æ•°è·å–"""
        current_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆ5åˆ†é’Ÿï¼‰
        if (self._t_param_cache and 
            current_time - self._t_param_time < self._t_param_timeout):
            print("â™»ï¸ ä½¿ç”¨ç¼“å­˜çš„tå‚æ•°")
            return self._t_param_cache
        
        # é‡æ–°è·å–tå‚æ•°
        print("ğŸ”„ é‡æ–°è·å–tå‚æ•°")
        t_value = self.get_t_parameter()
        if t_value:
            self._t_param_cache = t_value
            self._t_param_time = current_time
        return t_value    
    # å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜...
    def getName(self):
        pass

    def manualVideoCheck(self):
        pass

    def destroy(self):
        pass

    def _parse_video_item(self, item):
        """è§£æå•ä¸ªè§†é¢‘é¡¹"""
        try:
            #             # è·å–è§†é¢‘é“¾æ¥ é€‰æ‹©æ‰€æœ‰ <a> æ ‡ç­¾ï¼Œå¹¶ä¸”è¿™äº› <a> æ ‡ç­¾å…·æœ‰ v-item è¿™ä¸ªç±»  .attr('href') å°±æ˜¯è·å–è¿™ä¸ª <a> å…ƒç´ çš„ href å±æ€§
            #             a.v-item çš„å«ä¹‰
            # a.v-item è¿™ä¸ªCSSé€‰æ‹©å™¨çš„æ„æ€æ˜¯ï¼š

            # a: é€‰æ‹©æ‰€æœ‰ <a> æ ‡ç­¾ï¼ˆè¶…é“¾æ¥å…ƒç´ ï¼‰

            # .v-item: é€‰æ‹©æ‰€æœ‰å…·æœ‰ class="v-item" çš„å…ƒç´ 

                    # ç»„åˆèµ·æ¥: é€‰æ‹©æ‰€æœ‰æ—¢æ˜¯ <a> æ ‡ç­¾åˆå…·æœ‰ v-item ç±»çš„å…ƒç´ 
            vid = item.find('a.v-item').attr('href')
            if not vid:
                print("âŒ æœªæ‰¾åˆ°è§†é¢‘é“¾æ¥")
                return None



            # è·å–æ ‡é¢˜
            title = item.find('.v-item-title').eq(1).text().strip()

            cover = item.find(
                '.v-item-cover img').eq(1).attr('data-original')  # ç¬¬äºŒä¸ªimgæ˜¯å®é™…å›¾ç‰‡
            if not cover:
                cover = item.find('.v-item-cover img').attr('src')

            # è·å–è¯„åˆ†
            score_elem = item.find('.v-item-top-left span:contains("è±†ç“£")')
            score = score_elem.text().replace('è±†ç“£:', '').replace(
                'åˆ†', '').strip() if score_elem else ''

            # è·å–çŠ¶æ€/æ›´æ–°ä¿¡æ¯
            status = item.find('.v-item-bottom span').text().strip()

            return {
                'vod_id': vid,
                'vod_name': title,
                'vod_pic': "https://vres.zclmjc.com"+cover,
                'vod_remarks': status
            }

        except Exception as e:
            print(f"Error parsing video item: {e}")
            return None

    def homeContent(self, filter):
        try:

            response = requests.get(
                self.host, headers=self.headers, proxies=self.proxies)
            doc = pq(response.content)
            result = {}
            classes = []
            
            
            # å°è¯•ä¸åŒçš„é€‰æ‹©å™¨æ¥æ‰¾åˆ°åˆ†ç±»èœå•
            # æ–¹æ³•1: æŸ¥æ‰¾æ‰€æœ‰åŒ…å«åˆ†ç±»çš„èœå•
            # æŸ¥æ‰¾æ‰€æœ‰èœå•é¡¹

            menu_items = doc('.t-p-side-inner .menu-item')
            for item in menu_items.items():

                # è·å–èœå•æ–‡æœ¬

                label = item.find('.menu-item-label').text().strip()
                href = item.find('a').attr('href')

                # è¿‡æ»¤å‡ºéœ€è¦çš„åˆ†ç±»
                if label in ['ç”µå½±', 'è¿ç»­å‰§', 'åŠ¨æ¼«', 'ç»¼è‰ºçºªå½•', 'çŸ­å‰§', 'ä»Šæ—¥æ›´æ–°']:
                    # ä»é“¾æ¥ä¸­æå–ç±»å‹ID
                    classes.append({
                        'type_name': label,
                        'type_id': href
                    })
            result['class'] = classes

 # [{'type_name': 'ç”µå½±', 'type_id': '/channel/1.html'}, {'type_name': 'è¿ç»­å‰§', 'type_id': '/channel/2.html'}, {'type_name': 'åŠ¨æ¼«', 'type_id': '/channel/3.html'}, {'type_name': 'ç»¼è‰ºçºªå½•', 'type_id': '/channel/4.html'}, {'type_name': 'çŸ­å‰§', 'type_id': '/channel/6.html'}, {'type_name': 'ä»Šæ—¥æ›´æ–°', 'type_id': '/label/new.html'}]

    # æå–è§†é¢‘åˆ—è¡¨ - ä»å„ä¸ªæ¨¡å—ä¸­æå–
            video_list = []

            # æ–¹æ³•1: ä»"è¿‘æœŸçƒ­é—¨ç”µå½±"æ¨¡å—æå–
            movie_items = doc('.section-box:contains("è¿‘æœŸçƒ­é—¨ç”µå½±") .module-item')
            for item in movie_items.items():
                video_info = self._parse_video_item(item)
                if video_info:
                    video_list.append(video_info)

            # æ–¹æ³•2: ä»"è¿‘æœŸçƒ­é—¨å‰§é›†"æ¨¡å—æå–
            tv_items = doc('.section-box:contains("è¿‘æœŸçƒ­é—¨å‰§é›†") .module-item')
            for item in tv_items.items():
                video_info = self._parse_video_item(item)
                if video_info:
                    video_list.append(video_info)

            # æ–¹æ³•3: ä»"æœ€è¿‘æ›´æ–°"æ¨¡å—æå–
            update_items = doc('.section-box:contains("æœ€è¿‘æ›´æ–°") .module-item')
            for item in update_items.items():
                video_info = self._parse_video_item(item)
                if video_info:
                    video_list.append(video_info)

            result['list'] = video_list
            return result

        except Exception as e:
            print(f"é¦–é¡µå†…å®¹è·å–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'class': [], 'list': []}

    # å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜...
    def homeVideoContent(self):
        pass

    def _parse_category_video_item(self, item):
        """è§£æåˆ†ç±»é¡µé¢çš„å•ä¸ªè§†é¢‘é¡¹ - è¿”å›å•ä¸ªè§†é¢‘å­—å…¸"""
        try:
            # è·å–è§†é¢‘é“¾æ¥
            link = item.find('a.v-item').attr('href')
            if not link:
                return None

            # è·å–è§†é¢‘ID
            vid = link.split(
                '/detail/')[1].split('.')[0] if '/detail/' in link else ''

            # è·å–æ ‡é¢˜ - é€‰æ‹©ç¬¬äºŒä¸ª.v-item-titleï¼ˆç¬¬ä¸€ä¸ªå’Œç¬¬ä¸‰ä¸ªæ˜¯éšè—çš„ï¼‰
            title = item.find('.v-item-title').eq(1).text().strip()
            if not title:
                return None

            cover = item.find(
                '.v-item-cover img').eq(1).attr('data-original')  # ç¬¬äºŒä¸ªimgæ˜¯å®é™…å›¾ç‰‡
            if not cover:
                cover = item.find('.v-item-cover img').attr('src')

            # è·å–è¯„åˆ†
            score_elem = item.find('.v-item-top-left span:contains("è±†ç“£")')
            score = score_elem.text().replace('è±†ç“£:', '').replace(
                'åˆ†', '').strip() if score_elem else ''

            # è·å–æ›´æ–°çŠ¶æ€
            status = item.find('.v-item-bottom span').text().strip()

            # è¿”å›å•ä¸ªè§†é¢‘å­—å…¸
            return {
                'vod_id': link,  # ä½¿ç”¨å®Œæ•´é“¾æ¥ä½œä¸ºID
                'vod_name': title,
                'vod_pic': "https://vres.zclmjc.com/"+cover,
                'vod_remarks': status,
                'vod_tag': ''  # æ™®é€šè§†é¢‘æ²¡æœ‰ç‰¹æ®Šæ ‡ç­¾
            }

        except Exception as e:
            print(f"âŒ è§£æè§†é¢‘é¡¹å¤±è´¥: {e}")
            return None

    def getlist_category(self, data, tid=''):
        """ä¸“é—¨å¤„ç†åˆ†ç±»é¡µé¢çš„è§†é¢‘åˆ—è¡¨"""
        videos = []

        # å¦‚æœdataé€‰æ‹©å™¨æ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•ä½¿ç”¨æ–°çš„é€‰æ‹©å™¨
        if data.length != 0:
            # ä½¿ç”¨æ–°çš„é€‰æ‹©å™¨æŸ¥æ‰¾è§†é¢‘é¡¹
            video_items = data('.section-box .module-item')
            for item in video_items.items():
                video_info = self._parse_category_video_item(item)
                if video_info:
                    videos.append(video_info)

        return videos

    def categoryContent(self, tid, pg, filter, extend):
        print(f"ğŸ¯ åˆ†ç±»é¡µé¢è¯·æ±‚ï¼Œtid: {tid}, é¡µç : {pg}")

        # ğŸ†• åªå¤„ç†ç¬¬ä¸€é¡µï¼Œå…¶ä»–é¡µç è¿”å›ç©º
        if int(pg) != 1:
            print(f"âš ï¸ ç½‘ç«™ä¸æ”¯æŒåˆ†é¡µï¼Œè·³è¿‡ç¬¬{pg}é¡µè¯·æ±‚")
            return {'list': [], 'page': pg, 'pagecount': 1, 'limit': 0, 'total': 0}

        try:
            if '@folder' in tid:
                id = tid.replace('@folder', '')
                videos = self.getfod(id)
            else:
                url = f"{self.host}{tid}"
                print(f"ğŸŒ è¯·æ±‚URL: {url}")

                response = requests.get(
                    url, headers=self.headers, proxies=self.proxies)
                data = pq(response.content)
                videos = self.getlist_category(data, tid)

            print(f"âœ… æˆåŠŸè·å– {len(videos)} ä¸ªè§†é¢‘")

            return {
                'list': videos,
                'page': pg,
                'pagecount': 1,  # ğŸ†• æ˜ç¡®å‘Šè¯‰APPåªæœ‰1é¡µ
                'limit': len(videos),
                'total': len(videos)  # ğŸ†• ä½¿ç”¨å®é™…æ•°é‡
            }

        except Exception as e:
            print(f"âŒ åˆ†ç±»é¡µé¢è¯·æ±‚å¤±è´¥: {e}")
            return {'list': [], 'page': 1, 'pagecount': 1, 'limit': 0, 'total': 0}


#         vod_name (æ ‡é¢˜)

# vod_pic (å°é¢)

# vod_content (æè¿°)

# vod_director (å¯¼æ¼”)

# vod_actor (æ¼”å‘˜)

# vod_year (å¹´ä»½)

# vod_area (åœ°åŒº)

# type_name (ç±»å‹)

# vod_remarks (å¤‡æ³¨ï¼Œå¯èƒ½æ˜¯æ›´æ–°çŠ¶æ€)

# vod_play_from (æ’­æ”¾æºï¼Œå¤šä¸ªç”¨$$$åˆ†éš”)

# vod_play_url (æ’­æ”¾åˆ—è¡¨ï¼Œå¤šä¸ªæºç”¨$$$åˆ†éš”ï¼ŒåŒä¸€æºçš„å¤šé›†ç”¨#åˆ†éš”ï¼Œæ¯é›†æ ¼å¼ï¼šé›†å$URL)


    def detailContent(self, ids):
        """
        è·å–è§†é¢‘è¯¦æƒ…å†…å®¹
        ä¼˜åŒ–ç‚¹ï¼šæ›´å¥½çš„é”™è¯¯å¤„ç†ã€æ€§èƒ½ä¼˜åŒ–ã€ä»£ç ç»“æ„æ¸…æ™°åŒ–
        """
        try:
            # 1. URLå¤„ç†ä¼˜åŒ–
            url = ids[0] if ids[0].startswith(
                "http") else f"{self.host}{ids[0]}"

            # 2. è¯·æ±‚ä¼˜åŒ– - æ·»åŠ è¶…æ—¶å’Œé‡è¯•
            response = requests.get(url, headers=self.headers,
                                    proxies=self.proxies, timeout=10)
            response.raise_for_status()  # è‡ªåŠ¨å¤„ç†HTTPé”™è¯¯

            data = pq(response.content)
            vod = self._parse_vod_info(data, url)

            return {'list': [vod]}

        except requests.RequestException as e:
            print(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
            return {'list': [self._create_fallback_vod(ids[0])]}
        except Exception as e:
            print(f"è§£æè¯¦æƒ…é¡µå¤±è´¥: {e}")
            return {'list': [self._create_fallback_vod(ids[0])]}

    def _parse_vod_info(self, data, url):
        """è§£æè§†é¢‘åŸºæœ¬ä¿¡æ¯ - æ¨¡å—åŒ–å¤„ç†"""
        vod = {}

        # åŸºç¡€ä¿¡æ¯æå–
        vod.update(self._parse_basic_info(data))
        vod.update(self._parse_metadata(data))
        vod.update(self._parse_play_info(data, url))

        return vod

    def _parse_basic_info(self, data):
        """è§£æåŸºç¡€ä¿¡æ¯"""
        info = {}

        try:
            # æ ‡é¢˜ - ä½¿ç”¨æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨
            title_elem = data('.detail-title strong:nth-child(2)')
            info['vod_name'] = title_elem.text(
            ).strip() if title_elem else "æœªçŸ¥æ ‡é¢˜"

            # å°é¢å›¾ - ä¼˜åŒ–URLå¤„ç†
            cover_img = data('.detail-pic img')
            cover = cover_img.attr('data-original') or cover_img.attr('src')
            if cover:
                if not cover.startswith('http'):
                    cover = f"https://vres.zclmjc.com/{cover}"
            info['vod_pic'] = cover or ""

            # æè¿°
            desc_elem = data('.detail-desc p')
            info['vod_content'] = desc_elem.text(
            ).strip() if desc_elem else "æš‚æ— ç®€ä»‹"

        except Exception as e:
            print(f"è§£æåŸºç¡€ä¿¡æ¯å¤±è´¥: {e}")
            info.update({
                'vod_name': 'æœªçŸ¥æ ‡é¢˜',
                'vod_pic': '',
                'vod_content': 'æš‚æ— ç®€ä»‹'
            })

        return info

    def _parse_metadata(self, data):
        """è§£æå…ƒæ•°æ®ä¿¡æ¯"""
        info = {}

        try:
            # ä½¿ç”¨å­—å…¸æ˜ å°„æ›´æ¸…æ™°
            field_mapping = {
                'å¯¼æ¼”': 'vod_director',
                'æ¼”å‘˜': 'vod_actor',
                'shou': 'vod_year',
                'é¦–æ˜ ': 'vod_year',
                'åœ°åŒº': 'vod_area'
            }

            info_rows = data('.detail-info-row')
            for row in info_rows.items():
                label = row.find('.detail-info-row-side').text().strip()
                content = row.find('.detail-info-row-main').text().strip()

                for key, field in field_mapping.items():
                    if key in label and content:
                        info[field] = content
                        break

            # ç±»å‹æ ‡ç­¾ - ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼ä¼˜åŒ–
            tags = [tag.text().strip()
                    for tag in data('.detail-tags-item').items()]
            valid_tags = [tag for tag in tags if tag]
            info['type_name'] = ','.join(
                valid_tags[1:]) if valid_tags else "æœªçŸ¥"

            # æ›´æ–°çŠ¶æ€
            status_elem = data('.detail-info-row-main').eq(3)
            info['vod_remarks'] = status_elem.text(
            ).strip() if status_elem else "æ­£ç‰‡"

        except Exception as e:
            print(f"è§£æå…ƒæ•°æ®å¤±è´¥: {e}")
            info.update({
                'vod_director': '',
                'vod_actor': '',
                'vod_year': '',
                'vod_area': '',
                'type_name': 'æœªçŸ¥',
                'vod_remarks': 'æ­£ç‰‡'
            })

        return info

    def _parse_play_info(self, data, url):
        """è§£ææ’­æ”¾ä¿¡æ¯ - æ ¸å¿ƒä¼˜åŒ–"""
        info = {}

        try:
            play_sources = []
            source_episodes_list = []
                    # ğŸ†• å®šä¹‰è¦è·³è¿‡çš„çº¿è·¯å…³é”®è¯åˆ—è¡¨
            skip_keywords = ['4kå‘Šè¯‰ä¸å¡',  '4K(é«˜å³°ä¸å¡)', 'æµ‹è¯•çº¿è·¯']
        
            # è·å–æ‰€æœ‰æ’­æ”¾æº
            sources = data('.source-swiper-slide')

            for i, source in enumerate(sources.items()):
                source_name = source.find('.source-item-label').text().strip()
                if not source_name:
                    continue
            # ğŸ†• æ£€æŸ¥æ˜¯å¦åŒ…å«è¦è·³è¿‡çš„å…³é”®è¯
                skip_this_source = any(keyword in source_name for keyword in skip_keywords)
                if skip_this_source:
                    print(f"â­ï¸ è·³è¿‡çº¿è·¯: {source_name}")
                    continue

                play_sources.append(source_name)

                # è·å–å¯¹åº”å‰§é›†åˆ—è¡¨
                episodes = self._parse_episodes(data, i)
                if episodes:
                    source_episodes_list.append('#'.join(episodes))
                else:
                    # å¦‚æœæ²¡æœ‰å‰§é›†ï¼Œæ·»åŠ ç©ºåˆ—è¡¨å ä½
                    source_episodes_list.append('')

            # æœ€ç»ˆå¤„ç†
            if play_sources and any(source_episodes_list):
                info['vod_play_from'] = '$$$'.join(play_sources)
                info['vod_play_url'] = '$$$'.join(source_episodes_list)
            else:
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„æ’­æ”¾æºï¼Œä½¿ç”¨é»˜è®¤
                info['vod_play_from'] = 'é»˜è®¤'
                info['vod_play_url'] = f"æ­£ç‰‡${url}"

        except Exception as e:
            print(f"è§£ææ’­æ”¾ä¿¡æ¯å¤±è´¥: {e}")
            info.update({
                'vod_play_from': 'é»˜è®¤',
                'vod_play_url': f"æ­£ç‰‡${url}"
            })

        return info

    def _parse_episodes(self, data, source_index):
        """è§£æå‰§é›†åˆ—è¡¨ - ç‹¬ç«‹æ–¹æ³•ä¾¿äºç»´æŠ¤"""
        episodes = []

        try:
            episode_lists = data('.episode-list')
            if source_index < len(episode_lists):
                episode_list = episode_lists.eq(source_index)

                for episode in episode_list.find('.episode-item').items():
                    episode_name = episode.text().strip()
                    episode_url = episode.attr('href')

                    if episode_url and episode_name:
                        if not episode_url.startswith('http'):
                            episode_url = f"{self.host}{episode_url}"
                        episodes.append(f"{episode_name}${episode_url}")

        except Exception as e:
            print(f"è§£æç¬¬{source_index}ä¸ªæ’­æ”¾æºçš„å‰§é›†å¤±è´¥: {e}")

        return episodes

    def _create_fallback_vod(self, url):
        """åˆ›å»ºå›é€€çš„vodä¿¡æ¯"""
        return {
            'vod_name': 'åŠ è½½å¤±è´¥',
            'vod_pic': '',
            'vod_content': 'è¯¦æƒ…åŠ è½½å¤±è´¥ï¼Œè¯·é‡è¯•',
            'vod_director': '',
            'vod_actor': '',
            'vod_year': '',
            'vod_area': '',
            'type_name': 'æœªçŸ¥',
            'vod_remarks': 'åŠ è½½å¤±è´¥',
            'vod_play_from': 'é»˜è®¤',
            'vod_play_url': f"æ­£ç‰‡${url}"
        }
    def searchContent(self, key, quick, pg="1"):
        """ä¼˜åŒ–æœç´¢æ€§èƒ½ - ä½¿ç”¨ç¼“å­˜tå‚æ•° + å¿«é€Ÿè§£æ"""
        try:
            start_time = time.time()
            
            # 1. è·å–tå‚æ•°ï¼ˆå¸¦ç¼“å­˜ï¼‰
            t_value = self.get_t_parameter_cached()
            if not t_value:
                print("âŒ æ— æ³•è·å–tå‚æ•°ï¼Œæœç´¢å¤±è´¥")
                return {'list': [], 'page': pg}

            # 2. ä½¿ç”¨tå‚æ•°æœç´¢
            result_html = self.search_with_t(t_value, key)
            if not result_html:
                print("âŒ æœç´¢è¯·æ±‚å¤±è´¥")
                return {'list': [], 'page': pg}

            # 3. å¿«é€Ÿè§£ææœç´¢ç»“æœ
            search_data = self.parse_search_results_fast(result_html)
            
            # 4. è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            videos = []
            for item in search_data.get('items', []):
                video_item = {
                    'vod_id': item.get('detail_url', '').replace(self.host, ''),
                    'vod_name': item.get('title', ''),
                    'vod_pic': "https://vres.zclmjc.com/" + item.get('image_url', ''),
                    'vod_remarks': item.get('year', '') or item.get('type', ''),
                    'vod_content': f"{item.get('region', '')} {item.get('genres', '')} {item.get('actors', '')}"
                }
                # è¿‡æ»¤ç©ºå€¼
                video_item = {k: v for k, v in video_item.items() if v}
                if video_item.get('vod_id') and video_item.get('vod_name'):
                    videos.append(video_item)

            elapsed = time.time() - start_time
            print(f"âœ… æœç´¢å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’ï¼Œç»“æœæ•°: {len(videos)}")
            
            return {
                'list': videos, 
                'page': pg, 
                'pagecount': 99999, 
                'total': search_data.get('total_results', 0)
            }

        except Exception as e:
            print(f"âŒ æœç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return {'list': [], 'page': pg}

    def parse_search_results_fast(self, html_content):
        """å¿«é€Ÿè§£ææœç´¢ç»“æœ - ä¼˜åŒ–æ€§èƒ½"""
        data = pq(html_content)
        results = {
            'total_results': 0,
            'current_page_results': 0,
            'items': []
        }

        # å¿«é€Ÿæå–æ€»æ•°
        result_info = data('.search-result-info')
        if result_info:
            result_text = result_info.text()
            match = re.search(r'æ‰¾åˆ°\s*(\d+)\s*éƒ¨å½±ç‰‡', result_text)
            if match:
                results['total_results'] = int(match.group(1))

        # å¿«é€Ÿè§£ææ¯ä¸ªé¡¹ç›®
        search_items = data('.search-result-item')
        
        for item in search_items.items():
            try:
                
                    
                detail_url = item.attr('href') or ''
                if detail_url and not detail_url.startswith('http'):
                    detail_url = "https://103.194.185.51:51123" + detail_url

                # æ ‡é¢˜
                title_elem = item.find('.title')
                title = title_elem.text() if title_elem else "æœªçŸ¥æ ‡é¢˜"

                # å›¾ç‰‡
                img_elem = item.find('img.lazy').eq(0)
                image_url = img_elem.attr('data-original') or img_elem.attr('src') or ''

                # å…¶ä»–ä¿¡æ¯ä¸€æ¬¡æ€§è·å–
                tags_elem = item.find('.tags')
                tags_text = tags_elem.text() if tags_elem else ""
                tags_parts = tags_text.split('/') if tags_text else []
                
                year = tags_parts[0].strip() if len(tags_parts) > 0 else ""
                region = tags_parts[1].strip() if len(tags_parts) > 1 else ""
                genres = tags_parts[2].strip() if len(tags_parts) > 2 else ""

                # æ¼”å‘˜
                actors_elem = item.find('.actors')
                actors = actors_elem.text() if actors_elem else ""

                results['items'].append({
                    'title': title,
                    'detail_url': detail_url,
                    'image_url': image_url,
                    'year': year,
                    'region': region,
                    'genres': genres,
                    'actors': actors
                })
                
            except Exception as e:
                print(f"è§£ææœç´¢é¡¹å¤±è´¥: {e}")
                continue

        results['current_page_results'] = len(results['items'])
        return results


    def get_t_parameter(self):
        """ä»é¦–é¡µè·å– t å‚æ•°å€¼ - ç°åœ¨ä½¿ç”¨åˆå§‹åŒ–æ—¶ç”Ÿæˆçš„cookie"""
        url = "https://103.194.185.51:51123"

        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()

            # ä½¿ç”¨ pyquery è§£æ HTML
            data = pq(response.text)
            t_input = data('input[name="t"]')

            if t_input and t_input.attr('value'):
                t_value = t_input.attr('value')
                print(f"æˆåŠŸè·å– t å‚æ•°: {t_value}")
                return t_value
            else:
                print("æœªæ‰¾åˆ° t å‚æ•°")
                return None

        except requests.RequestException as e:
            print(f"è·å– t å‚æ•°è¯·æ±‚å¤±è´¥: {e}")
            return None

    def search_with_t(self, t_value, keyword):
        """ä½¿ç”¨è·å–çš„ t å‚æ•°è¿›è¡Œæœç´¢ - ä½¿ç”¨åˆå§‹åŒ–æ—¶ç”Ÿæˆçš„cookie"""
        url = "https://103.194.185.51:51123/search"
        params = {
            "t": t_value,
            "k": keyword
        }

        try:
            response = requests.get(
                url, headers=self.headers, params=params, timeout=10)
            response.raise_for_status()

            print(f"æœç´¢è¯·æ±‚çŠ¶æ€ç : {response.status_code}")
            print(f"æœç´¢URL: {response.url}")

            return response.text

        except requests.RequestException as e:
            print(f"æœç´¢è¯·æ±‚å¤±è´¥: {e}")
            return None

    # å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜...
   
   

    def playerContent(self, flag, id, vipFlags):
        """
        æ’­æ”¾æ—¶ä½¿ç”¨WebDecryptorè§£æçœŸå®è§†é¢‘åœ°å€
        """
        print(f"ğŸ¬ å¼€å§‹è§£ææ’­æ”¾åœ°å€: {id}")

        try:
            # å¦‚æœå·²ç»æ˜¯è§†é¢‘æ ¼å¼ï¼Œç›´æ¥è¿”å›
            if self.isVideoFormat(id):
                print(f"âœ… å·²æ˜¯è§†é¢‘æ ¼å¼ï¼Œç›´æ¥è¿”å›: {id}")
                return {
                    'parse': 0,
                    'url': id,
                    'header': self.headers
                }

            # ä½¿ç”¨WebDecryptorè§£ææ’­æ”¾é¡µé¢è·å–çœŸå®è§†é¢‘åœ°å€
            real_video_url = self._parse_play_page_with_decryptor(id)

            if real_video_url:
                print(f"âœ… è§£æåˆ°çœŸå®è§†é¢‘åœ°å€: {real_video_url}")

                # åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥è§£æ
                if self.isVideoFormat(real_video_url):
                    return {
                        'parse': 0,  # ç›´æ¥æ’­æ”¾
                        'url': real_video_url,
                        'header': self.headers
                    }
                else:
                    return {
                        'parse': 1,  # éœ€è¦è¿›ä¸€æ­¥è§£æ
                        'url': real_video_url,
                        'header': self.headers
                    }
            else:
                print(f"âŒ æ— æ³•è§£ææ’­æ”¾åœ°å€ï¼Œè¿”å›åŸå§‹åœ°å€")
                return {
                    'parse': 1,
                    'url': id,
                    'header': self.headers
                }

        except Exception as e:
            print(f"âŒ æ’­æ”¾åœ°å€è§£æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'parse': 1,
                'url': id,
                'header': self.headers
            }

    def _parse_play_page_with_decryptor(self, play_page_url):
        """
        ä½¿ç”¨WebDecryptorè§£ææ’­æ”¾é¡µé¢ï¼Œæå–çœŸå®è§†é¢‘åœ°å€
        """
        try:
            print(f"ğŸ” ä½¿ç”¨è§£å¯†å™¨è§£ææ’­æ”¾é¡µé¢: {play_page_url}")
            
            # ç¡®ä¿URLå®Œæ•´
            if not play_page_url.startswith('http'):
                play_page_url = f"{self.host}{play_page_url}"
            
            # åˆ›å»ºè§£å¯†å™¨å®ä¾‹ - ä½¿ç”¨ä½ åŸæ¥çš„cookie
            decryptor = WebDecryptor()
            
            # ä½¿ç”¨spiderçš„cookie
            if hasattr(self, 'current_cookie') and self.current_cookie:
                cookie_value = self.current_cookie.split('=')[1] if '=' in self.current_cookie else self.current_cookie
                decryptor.cookies = {"cdndefend_js_cookie": cookie_value}
            
            # å¤„ç†URLè·å–m3u8åœ°å€
            result = decryptor.process_url(play_page_url)
            
            if result and result.get('m3u8_url'):
                m3u8_url = result['m3u8_url']
                print(f"âœ… æˆåŠŸè·å–m3u8åœ°å€: {m3u8_url}")
                
                # ç¡®ä¿URLæ˜¯ç»å¯¹è·¯å¾„
                if m3u8_url and not m3u8_url.startswith('http'):
                    if m3u8_url.startswith('//'):
                        m3u8_url = f"https:{m3u8_url}"
                    elif m3u8_url.startswith('/'):
                        m3u8_url = f"{self.host}{m3u8_url}"
                    else:
                        m3u8_url = f"{self.host}/{m3u8_url}"
                
                return m3u8_url
            else:
                print("âŒ è§£å¯†å™¨æœªèƒ½è·å–åˆ°m3u8åœ°å€")
                return None
            
        except Exception as e:
            print(f"âŒ ä½¿ç”¨è§£å¯†å™¨è§£ææ’­æ”¾é¡µé¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

    def isVideoFormat(self, url):
        """æ£€æŸ¥URLæ˜¯å¦æ˜¯è§†é¢‘æ ¼å¼"""
        if not url:
            return False
        return any(ext in url.lower() for ext in ['.m3u8', '.mp4', '.flv', '.ts', '.mkv', '.avi', '.mov', '.webm'])

    def localProxy(self, param):
        try:
            xtype = param.get('type', '')
            if 'm3u8' in xtype:
                path, url = unquote(param['pdid']).split('_dm_')
                data = requests.get(url, headers=self.headers,
                                    proxies=self.proxies, timeout=10).text
                lines = data.strip().split('\n')
                times = 0.0
                for i in lines:
                    if i.startswith('#EXTINF:'):
                        times += float(i.split(':')[-1].replace(',', ''))
                thread = threading.Thread(
                    target=self.some_background_task, args=(path, int(times)))
                thread.start()
                print('[INFO] è·å–è§†é¢‘æ—¶é•¿æˆåŠŸ', times)
                return [200, 'text/plain', data]
            elif 'xdm' in xtype:
                url = f"{self.host}{unquote(param['path'])}"
                res = requests.get(url, headers=self.headers,
                                   proxies=self.proxies, timeout=10).json()
                dms = []
                for k in res:
                    text = k.get('text')
                    children = k.get('children')
                    if text:
                        dms.append(text.strip())
                    if children:
                        for j in children:
                            ctext = j.get('text')
                            if ctext:
                                ctext = ctext.strip()
                                if "@" in ctext:
                                    dms.append(ctext.split(' ', 1)[-1].strip())
                                else:
                                    dms.append(ctext)
                return self.xml(dms, int(param['times']))
            url = self.d64(param['url'])
            match = re.search(r"loadBannerDirect\('([^']*)'", url)
            if match:
                url = match.group(1)
            res = requests.get(url, headers=self.headers,
                               proxies=self.proxies, timeout=10)
            return [200, res.headers.get('Content-Type'), res.content]
        except Exception as e:
            print(e)
            return [500, 'text/html', '']

    def some_background_task(self, path, times):
        try:
            time.sleep(1)
            purl = f"{self.getProxyUrl()}&path={quote(path)}&times={times}&type=xdm"
            self.fetch(
                f"http://127.0.0.1:9978/action?do=refresh&type=danmaku&path={quote(purl)}")
        except Exception as e:
            print(e)

    def xml(self, dms, times):
        try:
            tsrt = f'å…±æœ‰{len(dms)}æ¡å¼¹å¹•æ¥è¢­ï¼ï¼ï¼'
            danmustr = f'<?xml version="1.0" encoding="UTF-8"?>\n<i>\n\t<chatserver>chat.xtdm.com</chatserver>\n\t<chatid>88888888</chatid>\n\t<mission>0</mission>\n\t<maxlimit>99999</maxlimit>\n\t<state>0</state>\n\t<real_name>0</real_name>\n\t<source>k-v</source>\n'
            danmustr += f'\t<d p="0,5,25,16711680,0">{tsrt}</d>\n'
            for i in range(len(dms)):
                base_time = (i / len(dms)) * times
                dm0 = base_time + random.uniform(-3, 3)
                dm0 = round(max(0, min(dm0, times)), 1)
                dm2 = self.get_color()
                dm4 = re.sub(r'[<>&\u0000\b]', '', dms[i])
                tempdata = f'\t<d p="{dm0},1,25,{dm2},0">{dm4}</d>\n'
                danmustr += tempdata
            danmustr += '</i>'
            return [200, "text/xml", danmustr]
        except Exception as e:
            print(e)
            return [500, 'text/html', '']

    def get_color(self):
        # 10% æ¦‚ç‡éšæœºé¢œè‰², 90% æ¦‚ç‡ç™½è‰²
        if random.random() < 0.1:
            h = random.random()
            s = random.uniform(0.7, 1.0)
            v = random.uniform(0.8, 1.0)
            r, g, b = colorsys.hsv_to_rgb(h, s, v)
            r = int(r * 255)
            g = int(g * 255)
            b = int(b * 255)
            decimal_color = (r << 16) + (g << 8) + b
            return str(decimal_color)
        else:
            return '16777215'

    def e64(self, text):
        try:
            text_bytes = text.encode('utf-8')
            encoded_bytes = b64encode(text_bytes)
            return encoded_bytes.decode('utf-8')
        except Exception as e:
            print(f"Base64ç¼–ç é”™è¯¯: {str(e)}")
            return ""

    def d64(self, encoded_text):
        try:
            encoded_bytes = encoded_text.encode('utf-8')
            decoded_bytes = b64decode(encoded_bytes)
            return decoded_bytes.decode('utf-8')
        except Exception as e:
            print(f"Base64è§£ç é”™è¯¯: {str(e)}")
            return ""

    def gethosts(self):
        url = self.domin
        curl = self.getCache('host_51cn')
        if curl:
            try:
                data = pq(requests.get(curl, headers=self.headers,
                          proxies=self.proxies).content)('a').attr('href')
                if data:
                    parsed_url = urlparse(data)
                    url = parsed_url.scheme + "://" + parsed_url.netloc
            except:
                pass
        try:
            html = pq(requests.get(url, headers=self.headers,
                      proxies=self.proxies).content)
            html_pattern = r"Base64\.decode\('([^']+)'\)"
            html_match = re.search(html_pattern, html(
                'script').eq(-1).text(), re.DOTALL)
            if not html_match:
                raise Exception("æœªæ‰¾åˆ°html")
            html = pq(b64decode(html_match.group(1)).decode())(
                'script').eq(-4).text()
            return self.hstr(html)
        except Exception as e:
            print(f"è·å–ä¸»åŸŸåå¤±è´¥: {str(e)}")
            # self.log(f"è·å–: {str(e)}")
            return ""

    def getcnh(self):
        data = pq(requests.get(f"{self.host}/homeway.html",
                  headers=self.headers, proxies=self.proxies).content)
        url = data(
            '.post-content[itemprop="articleBody"] blockquote p').eq(0)('a').attr('href')
        parsed_url = urlparse(url)
        host = parsed_url.scheme + "://" + parsed_url.netloc
        self.setCache('host_51cn', host)

    def hstr(self, html):
        pattern = r"(backupLine\s*=\s*\[\])\s+(words\s*=)"
        replacement = r"\1, \2"
        html = re.sub(pattern, replacement, html)
        data = f"""
        var Vx = {{
            range: function(start, end) {{
                const result = [];
                for (let i = start; i < end; i++) {{
                    result.push(i);
                }}
                return result;
            }},

            map: function(array, callback) {{
                const result = [];
                for (let i = 0; i < array.length; i++) {{
                    result.push(callback(array[i], i, array));
                }}
                return result;
            }}
        }};

        Array.prototype.random = function() {{
            return this[Math.floor(Math.random() * this.length)];
        }};

        var location = {{
            protocol: "https:"
        }};

        function executeAndGetResults() {{
            var allLines = lineAry.concat(backupLine);
            var resultStr = JSON.stringify(allLines);
            return resultStr;
        }};
        {html}
        executeAndGetResults();
        """
        return self.p_qjs(data)

    def p_qjs(self, js_code):
        try:
            # ä½¿ç”¨execjsæ‰§è¡ŒJavaScriptä»£ç å¹¶è·å–å®é™…ç”Ÿæˆçš„åŸŸå
            try:

                # åˆ›å»ºJavaScriptæ‰§è¡Œç¯å¢ƒ
                ctx = execjs.compile(js_code)

                # æ‰§è¡Œè·å–åŸŸååˆ—è¡¨çš„å‡½æ•°
                result = ctx.call("executeAndGetResults")

                # è§£æJSONç»“æœ
                import json
                domains = json.loads(result)

                if domains and len(domains) > 0:
                    print("è·å–ä¸»é¡µæˆåŠŸ")
                    return domains

            except ImportError:
                self.log("execjsæœªå®‰è£…ï¼Œå°è¯•å…¶ä»–æ–¹æ³•")

            # å¦‚æœexecjsä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨subprocessè°ƒç”¨Node.js
            try:
                import subprocess
                import tempfile
                import os

                # åˆ›å»ºä¸´æ—¶JSæ–‡ä»¶
                with tempfile.NamedTemporaryFile(mode='w', suffix='.js', delete=False) as f:
                    # ä¿®æ”¹JSä»£ç ï¼Œä½¿å…¶ç›´æ¥è¾“å‡ºç»“æœ
                    modified_js = js_code.replace(
                        'executeAndGetResults();',
                        'console.log(executeAndGetResults());'
                    )
                    f.write(modified_js)
                    temp_file = f.name

                # ä½¿ç”¨Node.jsæ‰§è¡Œ
                result = subprocess.check_output(['node', temp_file],
                                                 stderr=subprocess.PIPE,
                                                 text=True)

                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.unlink(temp_file)

                # è§£æç»“æœ
                import json
                domains = json.loads(result.strip())
                return domains

            except Exception as e:
                self.log(f"Node.jsæ‰§è¡Œå¤±è´¥: {e}")

            # å¦‚æœä»¥ä¸Šæ–¹æ³•éƒ½å¤±è´¥ï¼Œå°è¯•ä»JSä»£ç ä¸­æå–å·²ç”Ÿæˆçš„åŸŸå
            import re

            # æŸ¥æ‰¾lineAryå’ŒbackupLineçš„å®šä¹‰
            lineAry_match = re.search(
                r'lineAry\s*=\s*(\[.*?\]);', js_code, re.DOTALL)
            backupLine_match = re.search(
                r'backupLine\s*=\s*(\[.*?\]);', js_code, re.DOTALL)

            domains = []

            # å°è¯•ç›´æ¥æå–lineAryçš„å€¼
            if lineAry_match:
                try:
                    lineAry_str = lineAry_match.group(1)
                    # ç®€å•çš„å­—ç¬¦ä¸²è§£æï¼Œæå–URL
                    urls = re.findall(r"https?://[^'\"]+", lineAry_str)
                    domains.extend(urls)
                except:
                    pass

            # å°è¯•ç›´æ¥æå–backupLineçš„å€¼
            if backupLine_match:
                try:
                    backupLine_str = backupLine_match.group(1)
                    # ç®€å•çš„å­—ç¬¦ä¸²è§£æï¼Œæå–URL
                    urls = re.findall(r"https?://[^'\"]+", backupLine_str)
                    domains.extend(urls)
                except:
                    pass

            # å¦‚æœæˆåŠŸæå–åˆ°åŸŸåï¼Œè¿”å›å®ƒä»¬
            if domains:
                return domains

            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
            self.log("æ— æ³•ä»JSä»£ç ä¸­æå–åŸŸå")
            return []

        except Exception as e:
            self.log(f"åŸŸåæå–å¤±è´¥: {e}")
            return []

    def get_domains(self):
        html = pq(requests.get(self.domin, headers=self.headers,
                  proxies=self.proxies).content)
        html_pattern = r"Base64\.decode\('([^']+)'\)"
        html_match = re.search(html_pattern, html(
            'script').eq(-1).text(), re.DOTALL)
        if not html_match:
            raise Exception("æœªæ‰¾åˆ°html")
        html = b64decode(html_match.group(1)).decode()
        words_pattern = r"words\s*=\s*'([^']+)'"
        words_match = re.search(words_pattern, html, re.DOTALL)
        if not words_match:
            raise Exception("æœªæ‰¾åˆ°words")
        words = words_match.group(1).split(',')
        main_pattern = r"lineAry\s*=.*?words\.random\(\)\s*\+\s*'\.([^']+)'"
        domain_match = re.search(main_pattern, html, re.DOTALL)
        if not domain_match:
            raise Exception("æœªæ‰¾åˆ°ä¸»åŸŸå")
        domain_suffix = domain_match.group(1)
        domains = []
        for _ in range(3):
            random_word = random.choice(words)
            domain = f"https://{random_word}.{domain_suffix}"
            domains.append(domain)
        return domains

    def getfod(self, id):
        url = f"{self.host}{id}"
        data = pq(requests.get(url, headers=self.headers,
                  proxies=self.proxies).content)
        vdata = data('.post-content[itemprop="articleBody"]')
        r = ['.txt-apps', '.line', 'blockquote', '.tags', '.content-tabs']
        for i in r:
            vdata.remove(i)
        p = vdata('p')
        videos = []
        for i, x in enumerate(vdata('h2').items()):
            c = i*2
            videos.append({
                'vod_id': p.eq(c)('a').attr('href'),
                'vod_name': p.eq(c).text(),
                'vod_pic': f"{self.getProxyUrl()}&url={self.e64(p.eq(c+1)('img').attr('data-xkrkllgl'))}",
                'vod_remarks': x.text()
            })
        return videos

    def host_late(self, url_list):
        if isinstance(url_list, str):
            urls = [u.strip() for u in url_list.split(',')]
        else:
            urls = url_list

        if len(urls) <= 1:
            return urls[0] if urls else ''

        results = {}
        threads = []

        def test_host(url):
            try:
                start_time = time.time()
                response = requests.head(
                    url, headers=self.headers, proxies=self.proxies, timeout=1.0, allow_redirects=False)
                delay = (time.time() - start_time) * 1000
                results[url] = delay
            except Exception as e:
                results[url] = float('inf')

        for url in urls:
            t = threading.Thread(target=test_host, args=(url,))
            threads.append(t)
            t.start()

        for t in threads:
            t.join()

        return min(results.items(), key=lambda x: x[1])[0]

    def getlist(self, data, tid=''):
        videos = []
        l = '/mrdg' in tid
        for k in data.items():
            a = k.attr('href')
            b = k('h2').text()
            c = k('span[itemprop="datePublished"]').text()
            if a and b and c:
                videos.append({
                    'vod_id': f"{a}{'@folder' if l else ''}",
                    'vod_name': b.replace('\n', ' '),
                    'vod_pic': f"{self.getProxyUrl()}&url={self.e64(k('script').text())}&type=img",
                    'vod_remarks': c,
                    'vod_tag': 'folder' if l else '',
                    'style': {"type": "rect", "ratio": 1.33}
                })
        return videos

    def aesimg(self, word):

        key = b64decode("Bp2ZFMpge+R67heFSoTDcNur2xa8pJACIOIvcK35pYU=")
        # JavaScriptçš„åå…­è¿›åˆ¶å­—ç¬¦ä¸²IVè½¬æ¢ä¸ºPython bytes
        iv = bytes.fromhex("6b6b7973313233343536373839303030")
        cipher = AES.new(key, AES.MODE_CBC, iv)
        decrypted = unpad(cipher.decrypt(word), AES.block_size)
        return decrypted
